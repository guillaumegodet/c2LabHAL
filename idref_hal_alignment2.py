import streamlit as st
import pandas as pd
import requests
import datetime
import time
import concurrent.futures
import unicodedata
import re
from io import BytesIO
from difflib import SequenceMatcher
from pydref import Pydref
from bs4 import BeautifulSoup 
from urllib.parse import urlencode 


# ----- optional fuzzy match -----
try:
    from rapidfuzz import fuzz
    USE_RAPIDFUZZ = True
except ImportError:
    USE_RAPIDFUZZ = False

try:
    import xlsxwriter
    EXCEL_ENGINE = "xlsxwriter"
except ImportError:
    try:
        import openpyxl
        EXCEL_ENGINE = "openpyxl"
    except ImportError:
        EXCEL_ENGINE = None 

# ===== CONFIG =====
st.set_page_config(page_title="Alignement Annuaire de chercheurs ‚Üî IdRef ‚Üî Collection HAL", layout="wide")

# ===== SIDEBAR =====
def add_sidebar_menu():
    st.sidebar.header("√Ä propos")
    st.sidebar.info(
        """
        **c2LabHAL - Alignement IdRef ‚Üî HAL**

        Cet outil permet :
        - de rechercher des correspondances entre des auteurs d‚Äôun fichier et IdRef,
        - d‚Äôextraire les formes-auteurs associ√©es √† des structures de recherche HAL,
        - et de fusionner les deux sources.

        Il peut √™tre utilis√© :
        - avec un fichier seul,
        - avec une collection HAL seule,
        - ou en combinant les deux.
        """
    )
    st.sidebar.markdown("---")
    st.sidebar.header("Autres applications c2LabHAL")
    st.sidebar.markdown("üìñ [c2LabHAL - Application Principale](https://c2labhal.streamlit.app/)")
    st.sidebar.markdown("üèõÔ∏è [Version Nantes Universit√©](https://c2labhal-nantes.streamlit.app/)")
    st.sidebar.markdown("üîó [Alignement IdRef ‚Üî HAL](https://c2labhal-idref-hal-alignment.streamlit.app/)")
    st.sidebar.markdown("---")
    st.sidebar.markdown("### Pr√©sentation du projet :")
    st.sidebar.markdown("[üìä Voir les diapositives](https://slides.com/guillaumegodet/deck-d5bc03#/2)")
    st.sidebar.markdown("### Code source :")
    st.sidebar.markdown("[üêô GitHub](https://github.com/GuillaumeGodet/c2labhal)")

add_sidebar_menu()

# ===== UTILS =====
def normalize_text(s):
    if s is None:
        return ""
    s = unicodedata.normalize("NFD", str(s))
    return "".join(ch for ch in s if unicodedata.category(ch) != "Mn").lower().strip()

def similarity_score(a, b):
    if USE_RAPIDFUZZ:
        # Utiliser un score plus strict pour un matching pr√©cis
        return fuzz.WRatio(a or "", b or "") 
    return SequenceMatcher(None, a or "", b or "").ratio() * 100

@st.cache_resource
def get_pydref():
    return Pydref()

pydref_api = get_pydref()

def search_idref_for_person(full_name, min_birth, min_death):
    try:
        return pydref_api.get_idref(
            query=full_name,
            min_birth_year=min_birth,
            min_death_year=min_death,
            is_scientific=True,
            exact_fullname=True,
        )
    except Exception:
        return []

# ===== HAL FUNCTIONS (Extraction des formes auteurs) =====
def fetch_publications_for_structures(struct_ids, year_min=None, year_max=None):
    """R√©cup√®re les publications li√©es √† une ou plusieurs structures HAL (structId_i)."""
    if isinstance(struct_ids, str):
        struct_ids = [s.strip() for s in struct_ids.split(",") if s.strip()]
    if not struct_ids:
        return []
    docs = []
    rows, start = 10000, 0
    year_min = year_min or 1900
    year_max = year_max or datetime.datetime.now().year
    struct_query = " OR ".join(struct_ids)
    base_q = f"structId_i:({struct_query}) AND producedDateY_i:[{year_min} TO {year_max}]"
    params = {"q": base_q, "wt": "json", "fl": "structHasAuthId_fs", "rows": rows}
    st.info(f"üîé Extraction des publications pour {', '.join(struct_ids)} ({year_min}-{year_max})...")
    while True:
        params["start"] = start
        r = requests.get("https://api.archives-ouvertes.fr/search/", params=params)
        r.raise_for_status()
        chunk = r.json().get("response", {}).get("docs", [])
        docs.extend(chunk)
        if len(chunk) < rows:
            break
        start += rows
        time.sleep(0.2)
    st.success(f"‚úÖ {len(docs)} publications trouv√©es.")
    return docs

def extract_author_ids(docs, struct_ids=None):
   
    ids = set()
    if isinstance(struct_ids, str):
        struct_ids = [s.strip() for s in struct_ids.split(",") if s.strip()]
    for d in docs:
        for entry in d.get("structHasAuthId_fs", []):
            try:
                struct_part = entry.split("_FacetSep_")[0]
                if struct_ids and struct_part not in struct_ids:
                    continue
                after_join = entry.split("_JoinSep_")[1]
                # CORRECTION : on r√©cup√®re l'ID complet de la forme auteur (le docid) pour inclure les INCOMING
                form_id = after_join.split("_FacetSep_")[0] 
                
                if form_id:
                    ids.add(form_id)
            except Exception:
                continue
    return list(sorted(ids))

def fetch_author_details_batch(ids, fields, batch_size=20):
    HAL_AUTHOR_API = "https://api.archives-ouvertes.fr/ref/author/"
    authors = []
    total = len(ids)
    prog = st.progress(0, text="üì¶ T√©l√©chargement des formes-auteurs HAL...")
    for i in range(0, total, batch_size):
        batch = ids[i:i+batch_size]
        # CORRECTION : Utilisation de docid au lieu de person_i pour interroger l'API
        # On ajoute docid dans les champs retourn√©s
        q = " OR ".join([f'docid:\"{x}\"' for x in batch]) 
        params = {"q": q, "wt": "json", "fl": fields + ",docid", "rows": batch_size}
        try:
            r = requests.get(HAL_AUTHOR_API, params=params)
            r.raise_for_status()
            authors += r.json().get("response", {}).get("docs", [])
        except Exception as e:
            st.warning(f"‚ö†Ô∏è Erreur HAL sur le lot {batch}: {e}")
        prog.progress(min((i+batch_size)/total, 1.0))
        time.sleep(0.25)
    prog.empty()
    return authors

# ===== IDREF enrichment (HAL - Parallel) =====

def process_hal_row(row, min_birth, min_death):
    """
    Tente l'enrichissement IdRef pour un auteur HAL.
    Priorit√© : 1. PPN(s) dans le champ idrefId_s 2. Recherche par nom.
    """
    hal_first = row.get("firstName_s") or ""
    hal_last = row.get("lastName_s") or ""
    hal_full = f"{hal_first} {hal_last}".strip()
    hal_idrefs = row.get("idrefId_s")

    result = {"idref_ppn_list": None, "idref_status": "not_found", "nb_match": 0,
              "match_info": None, "alt_names": None, "idref_orcid": None,
              "idref_description": None, "idref_idhal": None}
    
    # ----------------------------------------------------
    # 1. Tenter l'enrichissement via le PPN fourni par HAL (si disponible)
    # ----------------------------------------------------
    if pd.notna(hal_idrefs) and str(hal_idrefs).strip():
        # Extrait tous les PPN valides
        ppns = re.findall(r"([0-9]{6,}[A-ZX]?)", str(hal_idrefs))
        if ppns:
            descs, alts = [], []
            orcid, idhal, match_info = None, None, None
            parsed_any = False
            
            # Tente d'enrichir en r√©cup√©rant la notice compl√®te pour chaque PPN
            for ppn in ppns:
                try:
                    xml = pydref_api.get_idref_notice(ppn)
                    if not xml: continue
                    parsed_any = True
                    # Assurez-vous que l'impl√©mentation de Pydref supprime les namespaces XML
                    # Sinon, il faut faire xml.replace('xmlns="..."', '')
                    xml_clean = xml.replace('xmlns="http://www.loc.gov/MARC21/slim"', '')
                    soup = BeautifulSoup(xml_clean, "xml") 
                    
                    ids_details = pydref_api.get_identifiers_from_idref_notice(soup)
                    
                    # Extrait ORCID et IdHAL des identifiants d√©taill√©s
                    for ident in ids_details:
                        if "orcid" in ident and not orcid:
                            orcid = ident["orcid"]
                        if "idhal" in ident and not idhal:
                            idhal = ident["idhal"]
                    
                    # Extrait d'autres infos
                    desc = pydref_api.get_description_from_idref_notice(soup)
                    alt = pydref_api.get_alternative_names_from_idref_notice(soup)
                    nameinfo = pydref_api.get_name_from_idref_notice(soup)

                    if not match_info: # Garde le nom de la premi√®re notice
                        match_info = f"{nameinfo.get('first_name','')} {nameinfo.get('last_name','')}".strip()

                    if isinstance(desc, list): descs += desc
                    if isinstance(alt, list): alts += alt
                except Exception:
                    continue

            if parsed_any:
                result.update({
                    "idref_ppn_list": "|".join(ppns),
                    "idref_status": "found",
                    "nb_match": len(ppns),
                    "match_info": match_info,
                    "alt_names": "; ".join(sorted(set(alts))) if alts else None,
                    "idref_orcid": orcid,
                    "idref_description": "; ".join(descs) if descs else None,
                    "idref_idhal": idhal,
                })
                return result

    # ----------------------------------------------------
    # 2. Fallback : Recherche par nom dans IdRef (si pas de PPN ou √©chec de l'enrichissement PPN)
    # ----------------------------------------------------
    if hal_full:
        matches = search_idref_for_person(hal_full, min_birth, min_death)
        nb = len(matches)
        if nb > 0:
            ppns = [m.get("idref","").replace("idref","") for m in matches if m.get("idref")]
            descs, alts = [], []
            orcid, idhal, match_info = None, None, None
            for m in matches:
                # Le r√©sultat de search_idref_for_person inclut souvent des identifiants
                if isinstance(m.get("description"), list): descs += m["description"]
                if isinstance(m.get("alt_names"), list): alts += m["alt_names"]
                
                # Extraction des identifiants disponibles (IdHAL/ORCID)
                # Ces champs sont ajout√©s √† la racine du dict par la fonction get_idref corrig√©e
                if "orcid" in m and not orcid: orcid = m["orcid"]
                if "idhal" in m and not idhal: idhal = m["idhal"]
                    
                if not match_info:
                    match_info = f"{m.get('first_name','')} {m.get('last_name','')}".strip()
                    
            result.update({
                "idref_ppn_list": "|".join(ppns) if ppns else None,
                "idref_status": "found" if nb == 1 else "ambiguous",
                "nb_match": nb,
                "match_info": match_info,
                "alt_names": "; ".join(sorted(set(alts))) if alts else None,
                "idref_orcid": orcid,
                "idref_description": "; ".join(descs) if descs else None,
                "idref_idhal": idhal,
            })
            
    return result

def enrich_hal_rows_with_idref_parallel(hal_df, min_birth, min_death, max_workers=8):
    hal_df = hal_df.copy()
    # Ajout d'une colonne "auteur_hal_complet" pour l'onglet extraction
    hal_df['auteur_hal_complet'] = hal_df['firstName_s'] + ' ' + hal_df['lastName_s']
    st.info(f"üîÑ Enrichissement IdRef ({len(hal_df)} auteurs HAL)...")
    total = len(hal_df)
    results = []
    prog = st.progress(0)
    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as ex:
        futures={ex.submit(process_hal_row,row,min_birth,min_death):i for i,row in hal_df.iterrows()}
        done=0
        for fut in concurrent.futures.as_completed(futures):
            i=futures[fut]
            try:
                result_dict = fut.result()
                # Fusionner le r√©sultat avec les donn√©es HAL existantes
                hal_row = hal_df.iloc[i].to_dict()
                hal_row.update(result_dict)
                results.append(hal_row)
            except Exception as e:
                # G√©rer l'erreur si n√©cessaire
                results.append(hal_df.iloc[i].to_dict()) 
            done+=1
            if done%5==0 or done==total:prog.progress(done/total)
    prog.empty()
    return pd.DataFrame(results)

# ===== IDREF enrichment (FILE - Parallel) =====

def process_file_row(row, min_birth, min_death):
    """Effectue la recherche IdRef d√©taill√©e pour une seule ligne du fichier (utilis√© en parall√®le)."""
    first = str(row.get("Pr√©nom", "")).strip()
    last = str(row.get("Nom", "")).strip()
    full = f"{first} {last}".strip()
    matches = search_idref_for_person(full, min_birth, min_death)
    nb = len(matches)

    # R√©cup√©ration des donn√©es originales et initialisation des champs IdRef
    info = {c: row.get(c) for c in row.keys() if c not in ["Nom", "Pr√©nom"]}
    info.update({"Nom": last, "Pr√©nom": first, "idref_ppn_list": None, "idref_status": "not_found",
                "nb_match": nb, "match_info": None, "alt_names": None, "idref_orcid": None,
                "idref_description": None, "idref_idhal": None})

    if nb:
        ppns = [m.get("idref","").replace("idref","") for m in matches if m.get("idref")]
        info["idref_ppn_list"] = "|".join(ppns)
        info["idref_status"] = "found" if nb == 1 else "ambiguous"
        # Afficher tous les noms trouv√©s pour match_info
        info["match_info"] = "; ".join([f"{m.get('first_name','')} {m.get('last_name','')}" for m in matches])
        desc, alt = [], []
        orcid, idhal = None, None
        
        for m in matches:
            if isinstance(m.get("description"), list): desc += m["description"]
            if isinstance(m.get("alt_names"), list): alt += m["alt_names"]
            
            # Extraction des identifiants de la racine
            if "orcid" in m and not orcid: orcid = m["orcid"]
            if "idhal" in m and not idhal: idhal = m["idhal"]
        
        info["idref_description"] = "; ".join(desc) if desc else None
        info["alt_names"] = "; ".join(sorted(set(alt))) if alt else None
        info["idref_orcid"] = orcid
        info["idref_idhal"] = idhal
        
    return info

def enrich_file_rows_with_idref_parallel(df, min_birth, min_death, max_workers=8):
    """G√®re l'ex√©cution parall√®le de la recherche IdRef pour les lignes du fichier."""
    st.info(f"üîÑ Recherche IdRef parall√®le pour le fichier ({len(df)} auteurs)...")
    total = len(df)
    results = []
    prog = st.progress(0)
    
    # Convertir en liste de dictionnaires pour un acc√®s s√ªr par les threads
    rows_list = df.to_dict('records') 
    
    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as ex:
        futures = {ex.submit(process_file_row, row, min_birth, min_death): i for i, row in enumerate(rows_list)}
        done = 0
        for fut in concurrent.futures.as_completed(futures):
            try:
                results.append(fut.result())
            except Exception as e:
                st.warning(f"‚ö†Ô∏è Erreur lors du traitement d'un auteur: {e}")
            done += 1
            if done % 5 == 0 or done == total:
                prog.progress(done / total)
        
    prog.empty()
    st.success("Recherche IdRef termin√©e ‚úÖ")
    return pd.DataFrame(results)

# ===== FUZZY MERGE (Version corrig√©e et prioritaire) =====
def fuzzy_merge_file_hal(df_file, df_hal, threshold=90):
    """
    Fusion floue des auteurs du fichier avec les auteurs HAL.
    Priorise le statut PREFERRED et garantit un match unique (1:1).
    """
    # 1. Pr√©paration et normalisation des donn√©es
    df_file = df_file.copy()
    df_hal = df_hal.copy()
    
    # Colonnes HAL pertinentes, y compris le nouveau nom docid
    HAL_CORE_COLS = ["docid","person_i","lastName_s","firstName_s","valid_s","idHal_s","halId_s","idrefId_s","orcidId_s","emailDomain_s"]
    HAL_IDREF_COLS = ["idref_ppn_list", "idref_status", "nb_match", "match_info", "alt_names", "idref_orcid", "idref_description", "idref_idhal"]

    # Cr√©ation des colonnes normalis√©es pour la comparaison
    df_file["norm_full"] = (df_file["Pr√©nom"].fillna("").apply(normalize_text)+" "+df_file["Nom"].fillna("").apply(normalize_text)).str.strip()
    df_hal["norm_full"] = (df_hal["firstName_s"].fillna("").apply(normalize_text)+" "+df_hal["lastName_s"].fillna("").apply(normalize_text)).str.strip()
    df_hal["__matched"] = False
    
    # Priorisation des formes HAL: PREFERRED (2) > INCOMING (1)
    df_hal['__priority'] = df_hal['valid_s'].map({'PREFERRED': 2, 'INCOMING': 1}).fillna(0)
    df_hal = df_hal.sort_values(by=['__priority', 'norm_full'], ascending=[False, True]).reset_index(drop=True)
    
    merged = []
    st.info("üîÑ Tentative de fusion floue 1:1 (Fichier ‚Üí HAL, Priorit√© PREFERRED)...")
    prog = st.progress(0, text="Fusion...")
    total_file = len(df_file)
    
    for i, fr in df_file.iterrows():
        row = fr.to_dict()
        row["source"] = "Fichier"
        row["match_score"] = None
        
        best_score = -1
        best_match_hal = None
        f_norm = fr.get("norm_full","")

        if f_norm:
            # Recherche du meilleur match unique
            for i_hal, hr in df_hal[df_hal["__matched"] == False].iterrows():
                score = similarity_score(f_norm, hr.get("norm_full",""))
                
                # Crit√®re de s√©lection : Meilleur score et sup√©rieur au seuil
                if score >= threshold and score > best_score:
                    best_score = score
                    best_match_hal = hr

        # Si un match valide et unique est trouv√©
        if best_match_hal is not None:
            h = best_match_hal
            
            # Marquer la ligne HAL comme utilis√©e (pour garantir 1:1)
            df_hal.loc[h.name, "__matched"] = True
            
            # Remplacement/Ajout des donn√©es HAL
            for col in HAL_CORE_COLS:
                row[f"HAL_{col}"] = h.get(col)
            
            # Remplacement des infos IdRef du fichier par celles de HAL si HAL les a trouv√©es
            hal_ppn_found = pd.notna(h.get("idref_ppn_list"))
            for col in HAL_IDREF_COLS:
                if hal_ppn_found and pd.notna(h.get(col)):
                     row[col] = h.get(col)
                # Sinon, on garde la valeur IdRef du fichier (d√©j√† dans 'row')

            row["source"] = "Fichier + HAL"
            row["match_score"] = best_score
        
        # Supprimer les colonnes temporaires
        row.pop("norm_full", None)
        
        merged.append(row)
        prog.progress(min((i+1)/total_file, 1.0))
    
    prog.empty()

    # 2. Ajout des lignes HAL-only restantes
    st.info("‚ûï Ajout des auteurs HAL non-appari√©s...")
    
    # Colonnes sp√©cifiques au fichier (hors Nom, Pr√©nom, norm_full et les colonnes IdRef standard)
    file_specific_cols = [c for c in df_file.columns if c not in ["Nom", "Pr√©nom", "norm_full"] + HAL_IDREF_COLS]
    
    # Colonnes HAL avec pr√©fixe HAL_
    hal_pref_cols = [f"HAL_{c}" for c in HAL_CORE_COLS]
    
    for _, h in df_hal[df_hal["__matched"] == False].iterrows():
        # Ligne de base (initialis√©e √† None pour les colonnes Fichier)
        row = {c: None for c in file_specific_cols + hal_pref_cols + ["Nom", "Pr√©nom", "match_score", "source"]}

        # Ajout des donn√©es de base
        row["Nom"] = h.get("lastName_s") or ""
        row["Pr√©nom"] = h.get("firstName_s") or ""
        row["source"] = "HAL"
        
        # Ajout des donn√©es IdRef enrichies par HAL
        for col in HAL_IDREF_COLS:
             row[col] = h.get(col)

        # Ajout des donn√©es HAL
        for col in HAL_CORE_COLS:
            row[f"HAL_{col}"] = h.get(col)

        merged.append(row)

    # 3. Finalisation des colonnes pour l'onglet "R√©sultats"
    # COLONNES DEMAND√âES PAR L'UTILISATEUR
    CORE_REQUESTED_COLUMNS_ORDER = [
        "Nom", "Pr√©nom", "idref_ppn_list", "HAL_idrefId_s", "HAL_idHal_s", 
        "idref_idhal", "idref_orcid", "HAL_orcidId_s", "HAL_valid_s", 
        "HAL_docid", "HAL_emailDomain_s", "idref_description", "source"
    ]
    
    df_final = pd.DataFrame(merged)
    # Ajouter les colonnes sp√©cifiques au fichier √† l'ordre
    file_specific_cols_in_df = [c for c in file_specific_cols if c in df_final.columns]
    final_cols_order = ["Nom", "Pr√©nom"] + file_specific_cols_in_df + [c for c in CORE_REQUESTED_COLUMNS_ORDER if c not in ["Nom", "Pr√©nom"]] + ["match_score"]

    final_cols_to_keep = [c for c in final_cols_order if c in df_final.columns]
    df_final = df_final.loc[:, final_cols_to_keep]
    df_final = df_final.loc[:, ~df_final.columns.duplicated()]
    
    return df_final.sort_values(by=["Nom", "Pr√©nom"])


# ===== EXPORT (Mise √† jour des onglets et colonnes) =====
def export_xlsx(fusion, idref_df=None, hal_df=None, idref_hal_df_for_extraction=None, params=None):
    out = BytesIO()
    engine_to_use = EXCEL_ENGINE or "openpyxl"
    
        
    # 1. Onglet R√©sultats (fusion)
    FUSION_COLS = [
        "Nom", "Pr√©nom", "idref_ppn_list", "HAL_idrefId_s", "HAL_idHal_s", "idref_idhal", 
        "idref_orcid", "HAL_orcidId_s", "HAL_valid_s", "HAL_docid", "HAL_emailDomain_s", 
        "idref_description", "source", "match_score"
    ]
    
    # On ajoute les colonnes sp√©cifiques au fichier (Nom/Pr√©nom non inclus)
    file_specific_cols = [c for c in fusion.columns if c not in [
        "Nom", "Pr√©nom", "match_score"] + FUSION_COLS]
    FUSION_COLS_FINAL = [c for c in ["Nom", "Pr√©nom"] + file_specific_cols + FUSION_COLS if c in fusion.columns and c not in ["Nom", "Pr√©nom"]]
    FUSION_COLS_FINAL = ["Nom", "Pr√©nom"] + [c for c in FUSION_COLS_FINAL if c not in ["Nom", "Pr√©nom"]]
    
    # 2. Onglet Extraction IdRef (Fichier + HAL enrichis)
    IDREF_EXTRACTION_COLS = [
        "Nom", "Pr√©nom", "idref_ppn_list", "idref_status", "nb_match", "match_info", 
        "alt_names", "idref_orcid", "idref_description", "idref_idhal"
    ]
    
    # 3. Onglet Extraction HAL
    HAL_EXTRACTION_COLS = [
        "firstName_s", "lastName_s", "valid_s", "docid", "idHal_s", "orcidId_s", 
        "idrefId_s", "emailDomain_s", "idref_ppn_list", "idref_idhal", "idref_orcid" # on ajoute les champs enrichis pour le contexte
    ]

    with pd.ExcelWriter(out, engine=engine_to_use) as w:
        
        # Onglet R√©sultats
        fusion_to_export = fusion.loc[:, [c for c in FUSION_COLS_FINAL if c in fusion.columns]]
        fusion_to_export.to_excel(w, sheet_name="R√©sultats", index=False)
        
        # Onglet Extraction IdRef (toutes les entr√©es Fichier + HAL)
        if idref_hal_df_for_extraction is not None:
            # On conserve que les colonnes pertinentes √† l'extraction IdRef
            idref_extraction_to_export = idref_hal_df_for_extraction.loc[:, [c for c in IDREF_EXTRACTION_COLS if c in idref_hal_df_for_extraction.columns]]
            idref_extraction_to_export = idref_extraction_to_export.rename(columns={"nb_match": "idref_nb_match"})
            idref_extraction_to_export.to_excel(w, sheet_name="Extraction IdRef", index=False)
            
        # Onglet Extraction HAL
        if hal_df is not None:
            hal_extraction_to_export = hal_df.loc[:, [c for c in HAL_EXTRACTION_COLS if c in hal_df.columns]]
            # Renommage des colonnes HAL selon la demande
            rename_map = {
                "firstName_s": "HAL_firstName_s",
                "lastName_s": "HAL_lastName_s",
                "valid_s": "HAL_valid_s",
                "docid": "HAL_docid",
                "idHal_s": "HAL_idHal_s",
                "orcidId_s": "HAL_orcidId_s",
                "idrefId_s": "HAL_idrefId_s",
                "emailDomain_s": "HAL_emailDomain_s"
            }
            hal_extraction_to_export = hal_extraction_to_export.rename(columns=rename_map)
            hal_extraction_to_export.to_excel(w, sheet_name="Extraction HAL", index=False)
            
        if params is not None:
            pd.DataFrame([params]).to_excel(w, sheet_name="Param√®tres", index=False)
            
    out.seek(0)
    return out

# ... (Fonctions utilitaires et de traitement HAL/IdRef inchang√©es) ...

# ===== LAUNCHING THE APP (partie principale du script Streamlit) =====
if __name__ == "__main__":
    st.title("Alignement Auteurs Annuaire ‚Üî IdRef ‚Üî HAL")
    
    # ----------------------------------------------------
    # 1. PARAMETERS INPUT
    # ----------------------------------------------------
    with st.expander("‚öôÔ∏è Param√®tres d'ex√©cution et de recherche", expanded=True):
        col1, col2, col3 = st.columns(3)
        with col1:
            uploaded_file = st.file_uploader("üìÇ Uploader un fichier CSV/XLSX (Annuaire ou liste)", type=["csv", "xlsx"])
            file_provided = uploaded_file is not None
            if file_provided:
                df_preview = pd.read_csv(uploaded_file) if uploaded_file.name.endswith(".csv") else pd.read_excel(uploaded_file)
                st.info(f"{len(df_preview)} lignes charg√©es. Colonnes: {', '.join(df_preview.columns)}")
                col_nom_choice = st.selectbox("S√©lectionnez la colonne **Nom**:", df_preview.columns)
                col_pre_choice = st.selectbox("S√©lectionnez la colonne **Pr√©nom**:", df_preview.columns, index=0 if "Pr√©nom" in df_preview.columns else 0)
        
        with col2:
            structure_ids = st.text_area("üèõÔ∏è IDs Structures HAL (ex: UMR1234, CNRS_TOLOUSE)", height=100)
            hal_provided = bool(structure_ids.strip())
            year_min = st.number_input("Ann√©e min publication HAL", value=2015, min_value=1900, max_value=datetime.datetime.now().year)
            year_max = st.number_input("Ann√©e max publication HAL", value=datetime.datetime.now().year, min_value=1900, max_value=datetime.datetime.now().year)
            
        with col3:
            minb = st.number_input("Ann√©e de naissance min (IdRef)", value=1920, min_value=1900)
            mind = st.number_input("Ann√©e de d√©c√®s min (IdRef)", value=2005, min_value=1900)
            similarity_threshold = st.slider("Seuil de similarit√© (Fusion Nom/Pr√©nom)", min_value=70, max_value=100, value=90)
            threads = st.slider("Threads (Recherche parall√®le)", min_value=1, max_value=16, value=8)

    st.markdown("---")
    
    # ----------------------------------------------------
    # 2. EXECUTION LOGIC
    # ----------------------------------------------------

    if not file_provided and not hal_provided:
        st.warning("Veuillez uploader un fichier ou sp√©cifier des IDs de structure HAL pour commencer.")
        
    elif st.button("üöÄ Lancer l'alignement et l'extraction"):

        # MODE 1 ‚Äî FICHIER SEUL (enrichissement IdRef)
        if file_provided and not hal_provided:
            st.header("üîç Mode 1 : Fichier Annuaire seul (enrichissement IdRef)")
            
            df_in = df_preview.copy()
            df_in = df_in.rename(columns={col_nom_choice:"Nom", col_pre_choice:"Pr√©nom"})
            
            # Enrichissement IdRef
            idref_df = enrich_file_rows_with_idref_parallel(df_in, minb, mind, threads)
            
            # Affichage et Export
            st.dataframe(idref_df.head(20))
            
            # Cr√©ation de l'onglet Extraction IdRef (seulement le fichier ici)
            idref_extraction_to_export = idref_df.rename(columns={"nb_match": "idref_nb_match"})
            
            params = {"date": datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")}
            xlsx = export_xlsx(idref_df, idref_df=idref_df, hal_df=None, idref_hal_df_for_extraction=idref_extraction_to_export, params=params)
            st.download_button("‚¨áÔ∏è T√©l√©charger XLSX (Extraction IdRef)", xlsx, file_name="extraction_idref_fichier.xlsx")

        # MODE 2 ‚Äî HAL SEUL (extraction formes auteurs + enrichissement IdRef)
        elif hal_provided and not file_provided:
            st.header("üèõÔ∏è Mode 2 : Collection HAL seule (Extraction + Enrichissement IdRef)")
            
            # 1. Extraction HAL
            docs = fetch_publications_for_structures(structure_ids, year_min, year_max)
            author_ids = extract_author_ids(docs, structure_ids)
            st.info(f"‚ú® {len(author_ids)} formes-auteurs uniques trouv√©es. R√©cup√©ration des d√©tails...")
            
            HAL_FIELDS = ["lastName_s", "firstName_s", "valid_s", "idHal_s", "halId_s", "idrefId_s", "orcidId_s", "emailDomain_s"]
            hal_authors = fetch_author_details_batch(author_ids, HAL_FIELDS)
            hal_df = pd.DataFrame(hal_authors)
            
            # Nettoyage ORCID (au cas o√π il y a d'autres textes)
            if "orcidId_s" in hal_df.columns:
                def clean_orcid(val):
                    h = re.search(r"(\d{4}-\d{4}-\d{4}-\d{3}[0-9X])", str(val))
                    return h.group(1) if h else None
                hal_df["orcidId_s"] = hal_df["orcidId_s"].apply(clean_orcid)
            
            # 2. Enrichissement IdRef des auteurs HAL
            hal_df = enrich_hal_rows_with_idref_parallel(hal_df, minb, mind, threads)
            
            # Affichage et Export
            st.dataframe(hal_df.head(20))
            
            # Pr√©paration pour l'onglet Extraction IdRef (on renomme les colonnes)
            hal_for_idref_extraction = hal_df.rename(columns={
                "lastName_s": "Nom", "firstName_s": "Pr√©nom", "nb_match": "idref_nb_match"
            })

            params = {"structures": structure_ids, "year_min": year_min, "year_max": year_max,
                      "date": datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")}
            # On utilise le m√™me df pour l'onglet R√©sultats et l'onglet Extraction HAL
            xlsx = export_xlsx(hal_df, idref_df=None, hal_df=hal_df, idref_hal_df_for_extraction=hal_for_idref_extraction, params=params)
            st.download_button("‚¨áÔ∏è T√©l√©charger XLSX (Extraction HAL + IdRef)", xlsx, file_name="extraction_hal_idref.xlsx")
            
        # MODE 3 ‚Äî FUSION
        elif file_provided and hal_provided:
            st.header("üß© Mode 3 : Fichier + HAL (fusion compl√®te)")
            
            # 1. HAL extraction and enrichment 
            # ----------------------------------------------------
            # CODE MANQUANT R√âINT√âGR√â ICI
            # ----------------------------------------------------
            docs = fetch_publications_for_structures(structure_ids, year_min, year_max)
            author_ids = extract_author_ids(docs, structure_ids)
            st.info(f"‚ú® {len(author_ids)} formes-auteurs uniques trouv√©es. R√©cup√©ration des d√©tails...")
            
            HAL_FIELDS = ["lastName_s", "firstName_s", "valid_s", "idHal_s", "halId_s", "idrefId_s", "orcidId_s", "emailDomain_s"]
            hal_authors = fetch_author_details_batch(author_ids, HAL_FIELDS)
            hal_df = pd.DataFrame(hal_authors) # <--- hal_df est maintenant DEFINI

            # Nettoyage ORCID
            if "orcidId_s" in hal_df.columns:
                def clean_orcid(val):
                    h = re.search(r"(\d{4}-\d{4}-\d{4}-\d{3}[0-9X])", str(val))
                    return h.group(1) if h else None
                hal_df["orcidId_s"] = hal_df["orcidId_s"].apply(clean_orcid)

            # Enrichissement IdRef des auteurs HAL
            hal_df = enrich_hal_rows_with_idref_parallel(hal_df,minb,mind,threads) # <--- Ligne qui provoquait l'erreur
            
            # 2. File extraction and enrichment
            df_in = df_preview.copy()
            df_in = df_in.rename(columns={col_nom_choice:"Nom", col_pre_choice:"Pr√©nom"})
            idref_df = enrich_file_rows_with_idref_parallel(df_in, minb, mind, threads)
            
            # 3. Pr√©paration du DataFrame pour l'onglet "Extraction IdRef"
            
            # Pr√©paration des colonnes du fichier pour la concat√©nation
            # On ne garde que les colonnes pertinentes de l'enrichissement IdRef
            idref_df_base = idref_df[["Nom", "Pr√©nom", "idref_ppn_list", "idref_status", "nb_match", "match_info", "alt_names", "idref_orcid", "idref_description", "idref_idhal"]].copy()

            # On renomme les colonnes HAL pour matcher les colonnes Fichier (Nom/Pr√©nom/IdRef)
            hal_for_idref_extraction = hal_df.rename(columns={
                "lastName_s": "Nom", "firstName_s": "Pr√©nom"
            })
            # S√©lection des colonnes IdRef/Nom/Pr√©nom pour les auteurs HAL
            hal_for_idref_extraction = hal_for_idref_extraction[[
                 "Nom", "Pr√©nom", "idref_ppn_list", "idref_status", "nb_match", "match_info",
                 "alt_names", "idref_orcid", "idref_description", "idref_idhal"
            ]].copy()
            
            # Union des deux DataFrames pour l'onglet d'extraction
            idref_hal_df_for_extraction = pd.concat([idref_df_base, hal_for_idref_extraction], ignore_index=True)
            # Renommer la colonne 'nb_match' pour l'export
            idref_hal_df_for_extraction = idref_hal_df_for_extraction.rename(columns={"nb_match": "idref_nb_match"})

            # 4. Fuzzy Merge (Logique 1:1 et PREFERRED)
            st.info("‚öôÔ∏è Fusion floue...")
            # Note: on utilise idref_df (le fichier enrichi) et hal_df (le hal enrichi)
            fusion = fuzzy_merge_file_hal(idref_df, hal_df, threshold=similarity_threshold)
            st.dataframe(fusion.head(20))
            st.success("‚úÖ Fusion termin√©e")

            # 5. Export
            params = {"structures": structure_ids, "year_min": year_min, "year_max": year_max,
                      "similarity_threshold": similarity_threshold, "threads": threads,
                      "date": datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")}
            
            xlsx = export_xlsx(fusion, idref_df=None, hal_df=hal_df, idref_hal_df_for_extraction=idref_hal_df_for_extraction, params=params) 
            st.download_button("‚¨áÔ∏è T√©l√©charger XLSX fusion",xlsx,file_name="fusion_idref_hal.xlsx")

# ... (Fin du script) ...
